{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "---\n",
    "title: Allocative Bias Blog\n",
    "author: Dean Smith\n",
    "date: '2023-5-9'\n",
    "image: \"penguins.png\"\n",
    "description: \"In this blog post, I use New Jersey employment data to build a machine learning model and examine the model for potential bias. \"\n",
    "format: html\n",
    "---\n",
    "# Auditing Allocative Bias\n",
    "First, I import my dataset and packages I need for my research.\n",
    "from folktables import ACSDataSource, ACSIncome, BasicProblem, adult_filter\n",
    "import numpy as np\n",
    "\n",
    "STATE = \"NJ\"\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', \n",
    "                            horizon='1-Year', \n",
    "                            survey='person')\n",
    "\n",
    "acs_data = data_source.get_data(states=[STATE], download=True)\n",
    "\n",
    "acs_data.head()\n",
    "There is a lot of data here. I only want to use relevent features here so I will go ahead and clean my data.\n",
    "possible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\n",
    "acs_data[possible_features].head()\n",
    "\n",
    "\n",
    "I don't want to include Race or Employment status in my features so I will leave those out and get my features, labels, and group (Race).\n",
    "features_to_use = [f for f in possible_features if f not in ['RAC1P', \"ESR\"]]\n",
    "\n",
    "EmploymentProblem = BasicProblem(\n",
    "    features=features_to_use,\n",
    "    target='ESR',\n",
    "    target_transform=lambda x: x == 1,\n",
    "    group='RAC1P',\n",
    "    preprocess=lambda x: x,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")\n",
    "\n",
    "features, label, group = EmploymentProblem.df_to_numpy(acs_data)\n",
    "Lets see the shape of our features and the size of our dataset.\n",
    "for obj in [features, label, group]:\n",
    "  print(obj.shape)\n",
    "Now lets do a test/train split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features, label, group, test_size=0.2, random_state=0)\n",
    "Here, we put back our group (race) to groupby and our labels (employment status).\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(X_train, columns = features_to_use)\n",
    "df[\"group\"] = group_train\n",
    "df[\"label\"] = y_train\n",
    "\n",
    "len(df)\n",
    "From the output of the code cell below, we can see that about 48% of individuals in our training data are employed.\n",
    "df[\"label\"].mean()\n",
    "Now, lets see how many people are in each racial group in our training data. Race group 1 is white, race group 2 is black, and the rest are other self-identified racial groups.\n",
    "df.groupby(\"group\").size()\n",
    "Likewise, lets see what the employment rate for each racial group is.\n",
    "df.groupby(\"group\")[\"label\"].mean()\n",
    "Now lets visualize this data in a bar graph where we also show the different sex groups within the racial groups. Sex group 1 is male and sex group 2 is female.\n",
    "import seaborn as sns\n",
    "counts = df.groupby([\"group\", \"SEX\"])[\"label\"].mean().reset_index(name = \"mean\")\n",
    "sns.barplot(data = counts, x = \"group\", y = \"mean\", hue = \"SEX\")\n",
    "In the code cell below, we train our Logistic Regression model and see which number of polynomial features works best.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import warnings\n",
    "\n",
    "best_model = []\n",
    "best_score = -1\n",
    "best_dgegree = 0\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for deg in range(0, 4):\n",
    "\n",
    "        polynomial_logistic = Pipeline([\n",
    "            ('poly', PolynomialFeatures(degree=deg)),\n",
    "            ('logistic', LogisticRegression())\n",
    "        ])\n",
    "\n",
    "        iter_model = polynomial_logistic.fit(X_train, y_train)\n",
    "\n",
    "        acc = iter_model.score(X_train, y_train)\n",
    "\n",
    "        print(\"Degree of \" + str(deg) + \" -> Training Accuracy = \" + str(acc))\n",
    "\n",
    "        if acc > best_score:\n",
    "            best_score = acc\n",
    "            best_degree = deg\n",
    "            best_model.append(iter_model)\n",
    "\n",
    "print(\"Best Degree for Model: \" + str(best_degree))\n",
    "\n",
    "model = best_model[-1]\n",
    "\n",
    "Now lets get our model predictions for our testing data\n",
    "y_hat = model.predict(X_test)\n",
    "And now we see our model had a total accuracy of about 82%!\n",
    "(y_hat == y_test).mean()\n",
    "Below, we utilize a confusion matrix to get our model's PPV, FNR, and FPR.\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_hat, normalize='true').ravel()\n",
    "ppv = tp / (tp + fp)\n",
    "fnr = fn / (fn + tp)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(\"PPV: \" + str(ppv))\n",
    "print(\"FNR: \" + str(fnr))\n",
    "print(\"FPR: \" + str(fpr))\n",
    "Now lets see what our model's accuracy, PPV, FNR, and FPR are for both our white racial group and our black racial group.\n",
    "for group in range(1, 10):\n",
    "    try:\n",
    "        overall_acc = (y_hat == y_test)[group_test == group].mean()\n",
    "        test = y_test[group_test == group]\n",
    "        pred = y_hat[group_test == group]\n",
    "        tn, fp, fn, tp = confusion_matrix(test, pred, normalize='true').ravel()\n",
    "        ppv = tp / (tp + fp)\n",
    "        fnr = fn / (fn + tp)\n",
    "        fpr = fp / (fp + tn)\n",
    "\n",
    "        print(\"Group \" + str(group) + \" overall accuracy: \" + str(overall_acc)+ \" | PPV: \" + str(ppv) + \" | FNR: \" + str(fnr) + \" | FPR: \" + str(fpr) + \"\\n\")\n",
    "    except:\n",
    "        print(\"Group \" + str(group) + \" does not have sufficient enough data.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "As we can see, the model performs worse when dealing with black individuals as compared to white individuals. Additionally, the model performs better on white individuals than the total model average and worse on black individuals compared to the models average. However, the model had a higher PPV than the models average when predicting white individuals and a lower PPV when predicting black individuals. This means that when the model guesses that a white person is employed, the model correctly guesses it more than the average, but when the model guesses that a black person is employed, the model predicts worse than the average. In contrast, for black individuals, the model has a slightly higher FNR than the average meaning that when the model predicts a black person is unemployed then the model predicts wrong more than the average while the model's FNR for white indiviuals is about the average. Lastly,  \n",
    "Here, we put some data into our test dataframe for graphing later.\n",
    "import pandas as pd\n",
    "df_test = pd.DataFrame(X_test, columns = features_to_use)\n",
    "df_test[\"group\"] = group_test\n",
    "df_test[\"label\"] = y_test\n",
    "df_test[\"prediction\"] = y_hat\n",
    "\n",
    "len(df)\n",
    "Now lets see if our model is calibrated.\n",
    "df_test = df_test[df_test[\"group\"] < 3]\n",
    "\n",
    "means = df_test.groupby([\"group\", \"prediction\"])[\"label\"].mean().reset_index(name = \"mean\")\n",
    "sns.lineplot(data = means, x = \"prediction\", y = \"mean\", hue = \"group\")\n",
    "df_test[\"employed\"] = df_test[\"prediction\"] == 1\n",
    "\n",
    "means = df_test.groupby([\"group\", \"employed\"])[\"label\"].mean().reset_index(name = \"mean\")\n",
    "\n",
    "p = sns.barplot(data = means, x = \"employed\", y = \"mean\", hue = \"group\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
