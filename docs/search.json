[
  {
    "objectID": "posts/gradient-descent-project/index.html",
    "href": "posts/gradient-descent-project/index.html",
    "title": "Gradient Descent Blog",
    "section": "",
    "text": "In this blog, I will demonstrate how to implement gradient descent along with stochastic gradient descent. Gradient descent is the process in which we adjust the weights of our perceptron model thorugh the equation w = w - alpha * gradient. I will represent my findings using graphs along with explaining my processes throughout the blog.\nHere, I retrieve my data set be importing it through one of sklearn’s datasets. The data set chosen has two features (feature 1 and feature 2) which are displayed in the graph shown after running the code cell below. Additionally, I have also imported my logisticRegression source code which can be found using the following link: https://github.com/dmsmith25/dmsmith25.github.io/blob/7eeef82cbf16398e0d0e9900e31df7dd50b99e12/posts/gradient-descent-project/logisticRegression.py\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom logisticRegression import LogisticRegression\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn the code cell below, I initialize my logisticRegression instance LR using my imported source code. Then, I fit my logisticRegression model to my dataset. In my source code, the fit function uses gradient descent to adjust the weight of the logisticRegression model to accurately classify points from the dataset.\n\nLR = LogisticRegression()\n\nLR.fit(X, y, alpha=0.1, max_epochs=1000)\n\n# inspect the fitted value of w for LR\nLR.w \n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nTo see the performance of the logisticRegression model, the code cell below shows the line formed from the weights of the now fitted model and the emperical risk with respect to iterations in the fitting phase.\n\nloss = LR.prev_loss\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (-f1*LR.w[0])/LR.w[1], color = \"black\")\n\nnum_steps = len(LR.loss_history)\naxarr[1].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n#axarr[1].plot(LRS.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\nFinally, I introduce a modified form of gradient descent called stochastic gradient descent. In stochastic gradient descent, my model does not calculate the full gradient and instead calculates the gradient of batches of my dataset. Visually, the chart below shows the loss with respect to iterations between gradient descent and stochastic gradient descent.\n\nLRS = LogisticRegression()\nLRS.fit_stochastic(X, y, \n                  m_epochs = 1000,  \n                  batch_size = 10, \n                  alpha = 0.1)\n\nnum_steps = len(LRS.loss_history)\nplt.plot(np.arange(num_steps) + 1, LRS.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, \n                  max_epochs = 1000,\n                  alpha = 1.0)\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Iterations\")\nylab = plt.ylabel(\"Loss\")\n\nlegend = plt.legend() \n\n\n\n\nTo conclude, we found that through gradient descent we were able to minimize our emperical risk with our convex loss function sigmoid. We implemented gradient descent on some sample data and represented our findings in graphs. Lastly, we implemented stochastic gradient descent which calculates the partial gradient lightening the computation load on our program."
  },
  {
    "objectID": "posts/perceptron-project/test.html",
    "href": "posts/perceptron-project/test.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "p = Perceptron()\nmax_steps = 1000\np.fit(X, y, max_steps)\n\nprint(p.history[-10:])\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max,ax=None , style=\"solid\"):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  if ax is None:\n    if style == \"solid\":\n      plt.plot(x, y, color = \"black\")\n    if style == \"dashed\":\n      plt.plot(x, y, \"k--\")\n  else:\n    ax.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np.score(X, y)\n\n1.0\n\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.weight_history[600], -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")"
  },
  {
    "objectID": "posts/perceptron-project/index.html",
    "href": "posts/perceptron-project/index.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Runtime Analysis\nFor each update of the perceptron, it will take O(p) runtime because the number of datapoints is irrelevant when talking about just a singular update. In each update, the runtime is dependant on the number of features or columns of the matricies being multiplied.\nIn my findings, I was able to implement the perceptron algorithm and test it on various types of data. When data is linearly seperable, the perceptron will be able to classify data with 100% accuracy. If data is not linearly seperable then it will find the “line of best fit” for that particular dataset. The perceptron can be used on datasets with many features and the implementation will still stay the same but visualizing results will be increasingly difficult."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/penguins-project/index.html",
    "href": "posts/penguins-project/index.html",
    "title": "Penguins Blog",
    "section": "",
    "text": "In this blog, I will work with a dataset to classify penguins into 3 species. My dataset contains 17 potential features but in this blog my challenge is to use only 3 of these features to classify penguins at 100% accuracy. The purpose of this blog is to show how one might eliminate irrelevant features to save computation time and still give effective results.\nIn the code cell below, I import my pandas dependency and import my training dataset from a remote github repo.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nTo see the first 5 rows of my training dataset run the code cell below\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\nHere, I import my sklearn.preprocessing pacakge and prepare my data. This process includes hot encoding and dropping unnessecary features such as comments and individual ID.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n\n\n\nIn the code cell below, I import a combinations package to help me find all combinations of features where one is qualitative data and the other two are quantitative data. The purpose of this is to later test which combination of features yields the best performance in the classification model.\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncol_combos = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    col_combos.append(cols)\n  \n\nHere, I determine the best features and the training score of these features.\n\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\n\n\nwarnings.filterwarnings(\"ignore\") # I was getting a lot of warnings\n\nbest_score = 0.0\nbest_cols = [\"N/A\"]\n\nfor col in col_combos:\n    LR = LogisticRegression()\n    LR.fit(X_train[col], y_train)\n    score = LR.score(X_train[col], y_train)\n\n    if score > best_score:\n        best_score = score\n        best_cols = col\n\nprint(best_score)\nprint(best_cols)\n\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nNow, I get my model with the best features and train it for the testing phase.\n\nbest_cols = best_cols[(len(best_cols) - 2):] + best_cols[:(len(best_cols) - 2)]\n\nLR = LogisticRegression()\nLR.fit(X_train[best_cols], y_train)\nscore = LR.score(X_train[best_cols], y_train)\n\nprint(score)\n\n1.0\n\n\nIn the code cell below, I test my model against an imported testing dataset and print the testing score.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nIn the next two code cells, I prep the training data to only include the best features in order to show them visually.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nX_train[best_cols]\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      1\n      0\n      0\n    \n    \n      2\n      41.4\n      18.5\n      0\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      0\n      1\n      0\n    \n    \n      4\n      50.6\n      19.4\n      0\n      1\n      0\n    \n    \n      5\n      33.1\n      16.1\n      0\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      0\n      1\n      0\n    \n    \n      270\n      45.4\n      14.6\n      1\n      0\n      0\n    \n    \n      271\n      36.2\n      17.2\n      0\n      0\n      1\n    \n    \n      272\n      50.0\n      15.9\n      1\n      0\n      0\n    \n    \n      273\n      48.2\n      14.3\n      1\n      0\n      0\n    \n  \n\n256 rows × 5 columns\n\n\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFinally, we are able to visualize the performance of this model.\n\nplot_regions(LR, X_train[best_cols], y_train)\n\n\n\n\nIn our findings, we determined that Island, Culmen Length, and Culmen Depth were the best combination of features to classify penguins into the Adelie, Chinstrap, and Gentoo species. We were able to obtain a 100% training accuracy along with a 100% testing accuracy. Lastly, we represented the data visually and can see that our model effectively classifies penguins into these species."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A reflection of what I have learned from Timnit Gebru’s experience and talk with our class.\n\n\n\n\n\n\nApr 19, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog I build a model to classify different penguins\n\n\n\n\n\n\nApr 18, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog I build and implement a classification model using gradient descent\n\n\n\n\n\n\nApr 7, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog I build and implement a simple perceptron model\n\n\n\n\n\n\nFeb 22, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/linear-regression-project/index.html",
    "href": "posts/linear-regression-project/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import numpy as np\nclass LogisticRegression:\n\n    def __init__(self):\n        np.random.seed(123456)\n        self.w = np.random.rand(2)\n        self.loss_history = []\n        self.score_history = []\n        self.prev_loss = np.inf\n\n    def fit_analytic(self, X, y):\n        self.w = np.linalg.inv(X.T@X)@X.T@y\n\n    def fit_gradient(self, X, y, alpha, max_iter):\n        P = X.T@X\n        q = X.T@y\n\n        self.score_history.append(self.score(X,y))\n\n        for i in range(max_iter):\n            gradient = 2*(P@self.w - q)\n            self.w -= alpha*gradient\n\n            new_score = self.score(X,y)\n\n            self.score_history.append(new_score)\n\n\n    def predict(self, X):\n        return np.dot(X, self.w)\n\n    def score(self, X, y):\n        y_bar = np.full(shape=len(y), fill_value=(1/len(y))*sum(y), dtype=float)\n\n        return sum((self.predict(X) - y) ** 2) / sum((y_bar - y) ** 2)\n\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 2\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\nLR = LogisticRegression()\n\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(LR.w)\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nLR = LogisticRegression()\n\nLR.fit_gradient(X_train, y_train, 0.01, 1000)\n\nprint(LR.w)\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\n[1.61670494 1.02074476]\nTraining score = 0.4913\nValidation score = 0.523\n[1.61670494 1.02074476]\nTraining score = 0.4913\nValidation score = 0.523"
  },
  {
    "objectID": "posts/learning-from-Timnit-Gebru/index.html",
    "href": "posts/learning-from-Timnit-Gebru/index.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "Introduction\nDr. Timnit Gebru is a renowned computer and data scientist who has been instrumental in the recent push for ethical AI practices. Dr. Gebru immigrated from Ethiopia to the United States where she studied electrical engineering and received her PhD in computer vision at Stanford University.\nAfter school, Dr. Gebru had an incredibly successful career working with electrical hardware for big tech companies including Apple, Microsoft, and Google. Her departure from Google came after leaders in Google’s AI department refused to implement her requests for fair, unbiased, and ethical AI practices.\nAs an African woman in a field dominated by white men, Dr. Gebru’s perspective and work with Black in AI have pointed out numerous flaws and biases in big tech companies’ AI practices. Dr. Gebru’s work and bravery is truly inspiring and we are so thankful she has taken the time to speak with us.\n2020 Conference Talk Summary\nThroughout the talk, Dr. Gebru points out various flaws with AI systems that are used to make life-changing decisions. The flaws she points out vary from how the data was collected to the purpose of the model itself. In many instances, companies gather data of people without their consent or use these models to infringe on people’s rights.\nDr. Gebru also talks about her experience as an African woman in a field dominated by white men. She references boards of AI directors and pictures of conferences. In this way, Dr. Gebru is able to rationalize why some of these biases exist in these systems. It is clear to see why Dr. Gebru’s work has been so impactful given the lack of perspective in the computer science world.\nDr. Gebru also touches on the ignorance to blindly trust AI especially in situations where the decisions it is making are life-altering. An area she focuses on is law enforcement and when it comes to convictions or facial recognition, law enforcement is trusting the advice of AI more than it should.\nQuestions:\n\nAs aspiring computer/data scientists, what is one thing we can do to help fight these biases in AI?\nIn our class we have talked about how taking different statistics can lead to different conclusions about AI systems. During your Tutorial on FATE in Computer Vision talk in 2020, you referenced that there should be a regulatory agent like the FDA to monitor these systems. Is there any way we can ensure that such an agent does not add further bias to the problem?"
  }
]