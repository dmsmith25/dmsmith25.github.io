[
  {
    "objectID": "posts/linear-regression-project/index.html",
    "href": "posts/linear-regression-project/index.html",
    "title": "Linear Regression Blog Post",
    "section": "",
    "text": "In this blog post, I will implement least-squares linear regression and show its effectiveness along with experimenting on datasets with many features.\nFirst, I define some functions I will use later and import my packages along with my source code which can be fonud at:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom LinearRegression import LinearRegression\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nNext, I randomly create my dataset using the LR_data function defined in the cell above. With this dataset I use one feature in p_features so the data is easy to visualize and the implementation is simple.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nNow, I will define my models LR and LR2 to demonstrate the functionality of the least-squares linear regression and linear regression with gradient descent. As we can see, both produce the same weights along with training and validation score.\n\n\n\nLR = LinearRegression()\n\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(LR.w)\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, 0.005, 300)\n\nprint(LR2.w)\n\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\n[1.02648217 1.09309375]\nTraining score = 0.7294\nValidation score = 0.6914\n[1.02648217 1.09309375]\nTraining score = 0.7294\nValidation score = 0.6914\n\n\nHere is a plot of the progression for the training loss of my model I trained with gradient descent.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nNext, I will demonstrate the effectiveness of linear regression as the number of features in a dataset becomes larger. To do this, I have set up an algorithm to train a model and log its training and validation data for all number of features 1-99 (or n_train - 1). I have also printed out the last five validation scores to show variance of the model when the number features become increasingly larger.\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\nnum_features = range(1,100)\ntrain_score = []\nval_score = []\n\nfor r in range(1,100):\n    p_features = r\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR3 = LinearRegression()\n    LR3.fit_analytic(X_train, y_train)\n    train_score.append(LR3.score(X_train, y_train))\n    val_score.append(LR3.score(X_val, y_val))\n\nprint(val_score[-5:])\n\n\n\n\n    \n\n[0.6426486631930868, 0.6025571541658967, -0.04697862335287928, 0.7265468875728334, -0.01049629698831911]\n\n\nHere, I plot the results from my algorithm above. It makes sense that the training data would plateu as the number of features becomes larger and the validation data becomes more varied because this is a strong sign of overfitting.\n\nplt.plot(num_features, train_score, label='training data')\nplt.plot(num_features, val_score, label=\"validation data\")\nxlab = plt.xlabel(\"Number of Features\")\nylab = plt.ylabel(\"Score\")\n\n\n\n\nFinally, I want to show the effectiveness of LASSO regularization which uses a regularizing term to combat overfitting in overparameterized problems.\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nL.score(X_val, y_val)\n\n0.7148639800717893\n\n\nOut of my own curiousity, I wanted to see how LASSO regularization performs against the linear regression model as the number of features increases.\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\nnum_features = range(1,100)\ntrain_score = []\nval_score = []\n\nfor r in range(1,100):\n    p_features = r\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR3 = Lasso(alpha = 0.001)\n    LR3.fit(X_train, y_train)\n    train_score.append(LR3.score(X_train, y_train))\n    val_score.append(LR3.score(X_val, y_val))\n\nprint(val_score[-5:])\n\n[0.678936729525182, 0.7246032101771086, 0.8593997620941247, 0.8602776975346242, 0.7714380225770578]\n\n\nAs we can see in the graph below, LASSO regularization significantly reduces the variance in the validation data as the number of features in a dataset becomes larger.\n\nplt.plot(num_features, train_score, label='training data')\nplt.plot(num_features, val_score, label=\"validation data\")\nxlab = plt.xlabel(\"Number of Features\")\nylab = plt.ylabel(\"Score\")\n\n\n\n\nTo conclude, we were able to demonstrate the effectiveness of the least-squares linear regression in comparison with linear regression with gradient descent. We showed the danger of overfitting with linear regression as the amount of features in a dataset becomes larger compared the amount of data points. Lastly, we showed the effectiveness of LASO regularization which does a great job at minimizing the overfitting effects seen in linear regression when the number of features becomes too large relative to the amount of data points."
  },
  {
    "objectID": "posts/learning-from-Timnit-Gebru/index.html",
    "href": "posts/learning-from-Timnit-Gebru/index.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "After Dr. Gebru’s Talk\nDr. Gebru’s argument in her talk focused mainly on the inequities of advanced AI technologies specifically on how they relate to eugenics. Dr. Gebru started with a background on eugenics as a study and how it has always been looked at as a “science”. She cited numerous public figures in AI who use the same language as famous eugenics researchers from the 1900’s. Next, Dr. Gebru broke down the main eugenics talking points and how they eerily resemble those of genetic and human modifications with advanced AI and technology.\nAfter establishing the similarities between both eugenics and advanced human modifications, Dr. Gebru began to explain the inequities behind these new technologies. She gave insight into how only people with desirable or “fit” genetics and intellect will have access to such technologies. In this way, Dr. Gebru portrays the dangers of eugenics becoming a prominent idea amongst the creators of these advanced human modifications.\nIn a related but separate argument, Dr. Gebru dispels some of the hype around AGI and warns us about the dangers associated with AGI. She starts her argument by going through some notable computer scientists attempting to define what AGI is and none of them are able to produce a concrete answer. Here, Dr. Gebru makes her first point, which is that these big companies are trying to build something that they cannot clearly define themselves. Next, Dr. Gebru juxtaposes the rhetoric used to support AGI and that of religious texts. The point here is that the support of AGI is more similar to a religion than a science. In this way, Dr. Gebru explains how these companies are trying to build a god which can have some dire consequences.\nIn regards to the AGI argument, I agree with Dr. Gebru in some areas such as using machine learning models for specific use cases would be a much better option than a do-it-all program. In the talk, Dr. Gebru referenced a language translating model that did better than google translate which is utilized by Chat-GPT. I feel that this is a great example to show how a do-it-all program is not needed.\nAdditionally, I agree with Dr. Gebru’s cautiousness towards these big companies deciding how and on what data these large models are trained. The production of AGI reminds me of the 2008 housing crisis and how despite huge banks with rational and fiscally “responsible” people, their negligence has catastrophic consequences. I fear, along with Dr. Gebru, that putting all of our trust in businesses whose sole purpose is to increase share-holder value, is incredibly dangerous. Lastly, I feel that there should be a way for us to check these large companies to make sure they are being ethical and cautious in their development of AGI.\n\n\nReflection\nWhen working with Dr. Gebru and her work, I found it insightful how software models which make life-altering choices are designed by groups where minorities are underrepresented and therefore miss some of the biases from improper data-collection. When creating programs before my time with Dr. Gebru, I never really took into account any biases that could exist within the program. I feel that this part has been quite worrisome for me. I do not want to be involved in making a product that can disproportionately harm people based on factors they cannot control. In one of Dr. Gebru’s talks she mentioned having a department designed to make sure there are no biases in programs. I feel that this would be a great addition to the computer science world similar to how the FDA regulates food. An agency that all machine learning models must go through to publicize or use the model would greatly benefit those that are marginalized and also computer scientists for not having to harm people’s lives before realizing there are biases in their model."
  },
  {
    "objectID": "posts/gradient-descent-project/index.html",
    "href": "posts/gradient-descent-project/index.html",
    "title": "Gradient Descent Blog",
    "section": "",
    "text": "In this blog, I will demonstrate how to implement gradient descent along with stochastic gradient descent. Gradient descent is the process in which we adjust the weights of our perceptron model thorugh the equation w = w - alpha * gradient. I will represent my findings using graphs along with explaining my processes throughout the blog.\nHere, I retrieve my data set be importing it through one of sklearn’s datasets. The data set chosen has two features (feature 1 and feature 2) which are displayed in the graph shown after running the code cell below. Additionally, I have also imported my logisticRegression source code which can be found using the following link: https://github.com/dmsmith25/dmsmith25.github.io/blob/7eeef82cbf16398e0d0e9900e31df7dd50b99e12/posts/gradient-descent-project/logisticRegression.py\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom logisticRegression import LogisticRegression\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn the code cell below, I initialize my logisticRegression instance LR using my imported source code. Then, I fit my logisticRegression model to my dataset. In my source code, the fit function uses gradient descent to adjust the weight of the logisticRegression model to accurately classify points from the dataset.\n\nLR = LogisticRegression()\n\nLR.fit(X, y, alpha=0.1, max_epochs=1000)\n\n# inspect the fitted value of w for LR\nLR.w \n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nTo see the performance of the logisticRegression model, the code cell below shows the line formed from the weights of the now fitted model and the emperical risk with respect to iterations in the fitting phase.\n\nloss = LR.prev_loss\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (-f1*LR.w[0])/LR.w[1], color = \"black\")\n\nnum_steps = len(LR.loss_history)\naxarr[1].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n#axarr[1].plot(LRS.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\nFinally, I introduce a modified form of gradient descent called stochastic gradient descent. In stochastic gradient descent, my model does not calculate the full gradient and instead calculates the gradient of batches of my dataset. Visually, the chart below shows the loss with respect to iterations between gradient descent and stochastic gradient descent.\n\nLRS = LogisticRegression()\nLRS.fit_stochastic(X, y, \n                  m_epochs = 1000,  \n                  batch_size = 10, \n                  alpha = 0.1)\n\nnum_steps = len(LRS.loss_history)\nplt.plot(np.arange(num_steps) + 1, LRS.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, \n                  max_epochs = 1000,\n                  alpha = 1.0)\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Iterations\")\nylab = plt.ylabel(\"Loss\")\n\nlegend = plt.legend() \n\n\n\n\nTo conclude, we found that through gradient descent we were able to minimize our emperical risk with our convex loss function sigmoid. We implemented gradient descent on some sample data and represented our findings in graphs. Lastly, we implemented stochastic gradient descent which calculates the partial gradient lightening the computation load on our program."
  },
  {
    "objectID": "posts/perceptron-project/index.html",
    "href": "posts/perceptron-project/index.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Runtime Analysis\nFor each update of the perceptron, it will take O(p) runtime because the number of datapoints is irrelevant when talking about just a singular update. In each update, the runtime is dependant on the number of features or columns of the matricies being multiplied.\nIn my findings, I was able to implement the perceptron algorithm and test it on various types of data. When data is linearly seperable, the perceptron will be able to classify data with 100% accuracy. If data is not linearly seperable then it will find the “line of best fit” for that particular dataset. The perceptron can be used on datasets with many features and the implementation will still stay the same but visualizing results will be increasingly difficult."
  },
  {
    "objectID": "posts/penguins-project/index.html",
    "href": "posts/penguins-project/index.html",
    "title": "Penguins Blog",
    "section": "",
    "text": "In this blog, I will work with a dataset to classify penguins into 3 species. My dataset contains 17 potential features but in this blog my challenge is to use only 3 of these features to classify penguins at 100% accuracy. The purpose of this blog is to show how one might eliminate irrelevant features to save computation time and still give effective results.\nIn the code cell below, I import my pandas dependency and import my training dataset from a remote github repo.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nTo see the first 5 rows of my training dataset run the code cell below\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\nHere, I import my sklearn.preprocessing pacakge and prepare my data. This process includes hot encoding and dropping unnessecary features such as comments and individual ID.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n\n\n\nIn the code cell below, I import a combinations package to help me find all combinations of features where one is qualitative data and the other two are quantitative data. The purpose of this is to later test which combination of features yields the best performance in the classification model.\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncol_combos = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    col_combos.append(cols)\n  \n\nHere, I determine the best features and the training score of these features.\n\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\n\n\nwarnings.filterwarnings(\"ignore\") # I was getting a lot of warnings\n\nbest_score = 0.0\nbest_cols = [\"N/A\"]\n\nfor col in col_combos:\n    LR = LogisticRegression()\n    LR.fit(X_train[col], y_train)\n    score = LR.score(X_train[col], y_train)\n\n    if score > best_score:\n        best_score = score\n        best_cols = col\n\nprint(best_score)\nprint(best_cols)\n\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nNow, I get my model with the best features and train it for the testing phase.\n\nbest_cols = best_cols[(len(best_cols) - 2):] + best_cols[:(len(best_cols) - 2)]\n\nLR = LogisticRegression()\nLR.fit(X_train[best_cols], y_train)\nscore = LR.score(X_train[best_cols], y_train)\n\nprint(score)\n\n1.0\n\n\nIn the code cell below, I test my model against an imported testing dataset and print the testing score.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nIn the next two code cells, I prep the training data to only include the best features in order to show them visually.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nX_train[best_cols]\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      1\n      0\n      0\n    \n    \n      2\n      41.4\n      18.5\n      0\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      0\n      1\n      0\n    \n    \n      4\n      50.6\n      19.4\n      0\n      1\n      0\n    \n    \n      5\n      33.1\n      16.1\n      0\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      0\n      1\n      0\n    \n    \n      270\n      45.4\n      14.6\n      1\n      0\n      0\n    \n    \n      271\n      36.2\n      17.2\n      0\n      0\n      1\n    \n    \n      272\n      50.0\n      15.9\n      1\n      0\n      0\n    \n    \n      273\n      48.2\n      14.3\n      1\n      0\n      0\n    \n  \n\n256 rows × 5 columns\n\n\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFinally, we are able to visualize the performance of this model.\n\nplot_regions(LR, X_train[best_cols], y_train)\n\n\n\n\nIn our findings, we determined that Island, Culmen Length, and Culmen Depth were the best combination of features to classify penguins into the Adelie, Chinstrap, and Gentoo species. We were able to obtain a 100% training accuracy along with a 100% testing accuracy. Lastly, we represented the data visually and can see that our model effectively classifies penguins into these species."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dean Smith’s CSCI 0451 Blog",
    "section": "",
    "text": "In this blog I show the effectiveness of least-squares linear regression and LASSO regularization\n\n\n\n\n\n\nMay 2, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA reflection of what I have learned from Timnit Gebru’s experience and talk with our class.\n\n\n\n\n\n\nApr 19, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog I build a model to classify different penguins\n\n\n\n\n\n\nApr 18, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog I build and implement a classification model using gradient descent\n\n\n\n\n\n\nApr 7, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog I build and implement a simple perceptron model\n\n\n\n\n\n\nFeb 22, 2023\n\n\nDean Smith\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I made this blog in my CSCI 451 class to share my projects on Machine Learning topics."
  }
]