[
  {
    "objectID": "posts/linear-regression-project/index.html",
    "href": "posts/linear-regression-project/index.html",
    "title": "Linear Regression Blog Post",
    "section": "",
    "text": "In this blog post, I will implement least-squares linear regression and show its effectiveness along with experimenting on datasets with many features. Here is the link to my source code: https://github.com/dmsmith25/dmsmith25.github.io/blob/main/posts/linear-regression-project/LinearRegression.py\nFirst, I define some functions I will use later and import my packages along with my source code which can be fonud at:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom LinearRegression import LinearRegression\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nNext, I randomly create my dataset using the LR_data function defined in the cell above. With this dataset I use one feature in p_features so the data is easy to visualize and the implementation is simple.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nNow, I will define my models LR and LR2 to demonstrate the functionality of the least-squares linear regression and linear regression with gradient descent. As we can see, both produce the same weights along with training and validation score.\n\n\n\nLR = LinearRegression()\n\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(LR.w)\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, 0.005, 300)\n\nprint(LR2.w)\n\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\n[1.02648217 1.09309375]\nTraining score = 0.7294\nValidation score = 0.6914\n[1.02648217 1.09309375]\nTraining score = 0.7294\nValidation score = 0.6914\n\n\nHere is a plot of the progression for the training loss of my model I trained with gradient descent.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nNext, I will demonstrate the effectiveness of linear regression as the number of features in a dataset becomes larger. To do this, I have set up an algorithm to train a model and log its training and validation data for all number of features 1-99 (or n_train - 1). I have also printed out the last five validation scores to show variance of the model when the number features become increasingly larger.\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\nnum_features = range(1,100)\ntrain_score = []\nval_score = []\n\nfor r in range(1,100):\n    p_features = r\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR3 = LinearRegression()\n    LR3.fit_analytic(X_train, y_train)\n    train_score.append(LR3.score(X_train, y_train))\n    val_score.append(LR3.score(X_val, y_val))\n\nprint(val_score[-5:])\n\n\n\n\n    \n\n[0.6426486631930868, 0.6025571541658967, -0.04697862335287928, 0.7265468875728334, -0.01049629698831911]\n\n\nHere, I plot the results from my algorithm above. It makes sense that the training data would plateu as the number of features becomes larger and the validation data becomes more varied because this is a strong sign of overfitting.\n\nplt.plot(num_features, train_score, label='training data')\nplt.plot(num_features, val_score, label=\"validation data\")\nxlab = plt.xlabel(\"Number of Features\")\nylab = plt.ylabel(\"Score\")\n\n\n\n\nFinally, I want to show the effectiveness of LASSO regularization which uses a regularizing term to combat overfitting in overparameterized problems.\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nL.score(X_val, y_val)\n\n0.7148639800717893\n\n\nOut of my own curiousity, I wanted to see how LASSO regularization performs against the linear regression model as the number of features increases.\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\nnum_features = range(1,100)\ntrain_score = []\nval_score = []\n\nfor r in range(1,100):\n    p_features = r\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR3 = Lasso(alpha = 0.001)\n    LR3.fit(X_train, y_train)\n    train_score.append(LR3.score(X_train, y_train))\n    val_score.append(LR3.score(X_val, y_val))\n\nprint(val_score[-5:])\n\n[0.678936729525182, 0.7246032101771086, 0.8593997620941247, 0.8602776975346242, 0.7714380225770578]\n\n\nAs we can see in the graph below, LASSO regularization significantly reduces the variance in the validation data as the number of features in a dataset becomes larger.\n\nplt.plot(num_features, train_score, label='training data')\nplt.plot(num_features, val_score, label=\"validation data\")\nxlab = plt.xlabel(\"Number of Features\")\nylab = plt.ylabel(\"Score\")\n\n\n\n\nTo conclude, we were able to demonstrate the effectiveness of the least-squares linear regression in comparison with linear regression with gradient descent. We showed the danger of overfitting with linear regression as the amount of features in a dataset becomes larger compared the amount of data points. Lastly, we showed the effectiveness of LASO regularization which does a great job at minimizing the overfitting effects seen in linear regression when the number of features becomes too large relative to the amount of data points."
  },
  {
    "objectID": "posts/learning-from-Timnit-Gebru/index.html",
    "href": "posts/learning-from-Timnit-Gebru/index.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "After Dr. Gebru’s Talk\nDr. Gebru’s argument in her talk focused mainly on the inequities of advanced AI technologies specifically on how they relate to eugenics. Dr. Gebru started with a background on eugenics as a study and how it has always been looked at as a “science”. She cited numerous public figures in AI who use the same language as famous eugenics researchers from the 1900’s. Next, Dr. Gebru broke down the main eugenics talking points and how they eerily resemble those of genetic and human modifications with advanced AI and technology.\nAfter establishing the similarities between both eugenics and advanced human modifications, Dr. Gebru began to explain the inequities behind these new technologies. She gave insight into how only people with desirable or “fit” genetics and intellect will have access to such technologies. In this way, Dr. Gebru portrays the dangers of eugenics becoming a prominent idea amongst the creators of these advanced human modifications.\nIn a related but separate argument, Dr. Gebru dispels some of the hype around AGI and warns us about the dangers associated with AGI. She starts her argument by going through some notable computer scientists attempting to define what AGI is and none of them are able to produce a concrete answer. Here, Dr. Gebru makes her first point, which is that these big companies are trying to build something that they cannot clearly define themselves. Next, Dr. Gebru juxtaposes the rhetoric used to support AGI and that of religious texts. The point here is that the support of AGI is more similar to a religion than a science. In this way, Dr. Gebru explains how these companies are trying to build a god which can have some dire consequences.\nIn regards to the AGI argument, I agree with Dr. Gebru in some areas such as using machine learning models for specific use cases would be a much better option than a do-it-all program. In the talk, Dr. Gebru referenced a language translating model that did better than google translate which is utilized by Chat-GPT. I feel that this is a great example to show how a do-it-all program is not needed.\nAdditionally, I agree with Dr. Gebru’s cautiousness towards these big companies deciding how and on what data these large models are trained. The production of AGI reminds me of the 2008 housing crisis and how despite huge banks with rational and fiscally “responsible” people, their negligence has catastrophic consequences. I fear, along with Dr. Gebru, that putting all of our trust in businesses whose sole purpose is to increase share-holder value, is incredibly dangerous. Lastly, I feel that there should be a way for us to check these large companies to make sure they are being ethical and cautious in their development of AGI.\n\n\nReflection\nWhen working with Dr. Gebru and her work, I found it insightful how software models which make life-altering choices are designed by groups where minorities are underrepresented and therefore miss some of the biases from improper data-collection. When creating programs before my time with Dr. Gebru, I never really took into account any biases that could exist within the program. I feel that this part has been quite worrisome for me. I do not want to be involved in making a product that can disproportionately harm people based on factors they cannot control. In one of Dr. Gebru’s talks she mentioned having a department designed to make sure there are no biases in programs. I feel that this would be a great addition to the computer science world similar to how the FDA regulates food. An agency that all machine learning models must go through to publicize or use the model would greatly benefit those that are marginalized and also computer scientists for not having to harm people’s lives before realizing there are biases in their model."
  },
  {
    "objectID": "posts/final-project/index.html",
    "href": "posts/final-project/index.html",
    "title": "Final Project Blog",
    "section": "",
    "text": "Our group attempted to create a model that would take in a football players statistics for one season and would output a projected fantasy score for the following year. We wanted to create a model that could be trained on several years worth of data and would choose the model that was best for that dataset. We wanted split up each major fantasy football position into its own model in case there was one model that was better for one position vs. another. We wound up scraping our data online so it will be easy in the future to add in years and incorporate them into our training data. We ultimately were successful in creating this setup and obtaining results that seem relatively accurate to us. We would like to compare our results vs. other prediction algorithms out there for predicting fantasy football performance at some point.\nClick to see source code!"
  },
  {
    "objectID": "posts/final-project/index.html#introduction",
    "href": "posts/final-project/index.html#introduction",
    "title": "Final Project Blog",
    "section": "Introduction",
    "text": "Introduction\nOur project focused on developing a machine learning pipeline for fantasy football applications. All three of us have played fantasy football for a number of years, so we were motivated to see if modeling historic data could lead to accurate predictions for future seasons. For those who are unfamiliar, or need a refresher on what fantasy football is, here is a quick recap:\nFantasy Football\nThe primary goal of fantasy football is to select a fantasy team comprised of current NFL players. The standard roster includes one quarter back, two running backs, two wide receivers, one tight end, one kicker, one defensive/special teams unit, and one “flex,” which can be an additional running back, wide receiver, or tight end. There is also space for ~5-7 bench players whose points will not count if they remain on the bench. Here is an example roster:\n\nTypically, a fantasy football league will consist of 8-12 teams, and participants will battle head to head against there friends to see whose collective team performs better that week. The league will have playoffs towards the end of the season and eventually a championship.\nAs you will see in the “Week 1” columns on the right, there is one feature named “Proj,” which stands for projections. These metrics are very popular and commonly utilized in fantasy football, and team managers will often use them to compare different players and set their lineup each week. Like many, we have always been curious how these projections are generated. There have been several individuals and groups who have also tried to accomplish this task. For example, Chelsea Robinson at Louisiana tech wrote a case study in 2020 with her findings from advanced statistical modeling using historical fantasy data. Similar to us, she relied on regression modeling to output a ranking list for the following season. Although mathematically strong, her model uses less data and fewer features than ours, which might not produce as accurate as a result. Another interesting case study comes Roman Lutz at UMASS Amherst, who employed a similar solution as us. More specifically, he pulled data from over 5 seasons and used SVM regression along with neural networks for optimization. Similar to the first case study, his data was also fairly basic and lacked the advanced features found in ours. Consequently, his MSE was around 6, while ours was closer to 2. This is a significant error difference when it comes to prediction, so we are proud with our result. The last case study worth mentioning comes from Benjamin Hendricks at Harvard. In his approach, Hendricks uses an ensemble method to reach predictions. In his calculations, he leverages data from existing models, applies natural language processing techniques to perform sentiment analysis on player performance, and combines these metrics with standard data from NFL.com and FantasyData.io. Hendricks’s use of sentiment analysis and crowd sourcing is a unique approach and feature to include. He relies on the crowd’s opinion on players and teams instead of just the “masters.” He also includes advanced, real time statistics such as injuries and weather analysis. This is an impressive, detailed approach with great performance (30% better than most sport sites)."
  },
  {
    "objectID": "posts/final-project/index.html#values-statement",
    "href": "posts/final-project/index.html#values-statement",
    "title": "Final Project Blog",
    "section": "Values Statement",
    "text": "Values Statement\nPotential Users\nThe potential users of our project are fantasy football team owners. Our data, modeling, and output are all fairly specific, so there will not be many applications outside this domain. It is worth noting that our current output is specific to fantasy football drafts, which take place at the beginning of the year and allow users to pick their team for the year. If we had more time, we would have liked to model for weekly predictions.\nWho benefits?\nHopefully, the owners of fantasy football teams who leverage our product will gain increased insight and an edge over their opponents. These users can run our model for that given year and shape their draft off the results.\nWho is harmed?\nWhile no one will be truly harmed, this algorithm could provide an unfair advantage for certain members of a league. The algorithm should not be used if any sort of wagering is involved in the league, as this could cause for unfair and biased outcomes.\nWhat is your personal reason for working on this problem?\nAs aforementioned, we all have played fantasy football for a number of years and have been interested with how the projections are produced by major sites like ESPN and Yahoo. We wanted to see if we could replicate and expand on these predictions using the machine learning techniques we have explored this semester.\nSocietal Impact\nThere will be very little societal impact of our product. As we mentioned, it is a very specific application of machine learning, and it will primarily be used for fun instead of addressing any societal problems."
  },
  {
    "objectID": "posts/final-project/index.html#materials-and-methods",
    "href": "posts/final-project/index.html#materials-and-methods",
    "title": "Final Project Blog",
    "section": "Materials and Methods",
    "text": "Materials and Methods\n\n Our data\n\nNormal data\nWe wound up scraping most of our data online from various websites that provide NFL player statistics. We tested various websites but the one with the most data that was easily available to scrape was from a website called FantasyPros. This website has cleanly formatted NFL data for every player from each year. They also conveniently split up the players into positional groups, which made our job easier. Furthermore, the url for each position and year was structred in such a way that we could write the following function to web-scrape our basic data:\n\nimport pandas as pd\nimport requests\n\ndef read_new(year, pos):\n\n        # Link takes lowercase positions\n        pos = pos.lower()\n\n        url = f\"https://www.fantasypros.com/nfl/stats/{pos}.php?year={year}\"\n\n        response = requests.get(url)\n        html = response.content\n\n        # Make df\n        df = pd.read_html(html, header=1)[0]\n\n        # Clean name and team data\n\n        df.insert(1, 'Tm', df['Player'].str.rsplit(n=1).str[-1].str.slice(1, -1))\n        df['Player'] = df['Player'].str.rsplit(n=1).str[0]\n\n        # Get y (following year ppg)\n        next_year = str(int(year) + 1)\n        url = f\"https://www.fantasypros.com/nfl/stats/{pos}.php?year={next_year}\"\n\n        response = requests.get(url)\n        html = response.content\n\n        # Make df\n        y = pd.read_html(html, header=1)[0]\n\n        df['y'] = y['FPTS/G']\n\n        return df\n\nThis is what an example basic dataset looked like:\n\ndf = read_new(2021, \"QB\")\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      Rank\n      Tm\n      Player\n      CMP\n      ATT\n      PCT\n      YDS\n      Y/A\n      TD\n      INT\n      SACKS\n      ATT.1\n      YDS.1\n      TD.1\n      FL\n      G\n      FPTS\n      FPTS/G\n      ROST\n      y\n    \n  \n  \n    \n      0\n      1\n      BUF\n      Josh Allen\n      409\n      646\n      63.3\n      4407\n      6.8\n      36\n      15\n      26\n      122\n      763\n      6\n      3\n      17\n      417.7\n      24.6\n      99.9%\n      25.2\n    \n    \n      1\n      2\n      LAC\n      Justin Herbert\n      443\n      672\n      65.9\n      5014\n      7.5\n      38\n      15\n      31\n      63\n      302\n      3\n      1\n      17\n      395.6\n      23.3\n      96.6%\n      24.3\n    \n    \n      2\n      3\n      FA\n      Tom Brady\n      485\n      719\n      67.5\n      5316\n      7.4\n      43\n      12\n      22\n      28\n      81\n      2\n      3\n      17\n      386.7\n      22.7\n      1.8%\n      25.6\n    \n  \n\n\n\n\nAs you can see, each row represents a singular NFL player. In this case, we pulled QB data from 2021, so each row will represent a quarterback and their respective stats from that season. There are many features which display player performance throughout the season. Some example stats include ATT (pass attempts), YDS (passing yards), TD (touchdowns), CMP (completions). Our target variable, which we are trying to predict in future years, is FPTS/G: This is what it looks like:\n\ndf['FPTS/G']\n\n0     24.6\n1     23.3\n2     22.7\n3     22.0\n4     20.4\n      ... \n78    -0.3\n79    -0.1\n80    -0.2\n81    -0.5\n82    -0.4\nName: FPTS/G, Length: 83, dtype: float64\n\n\nWe decided on fantasy points per game instead of total fantasy points to account for injuries and other potential limitations of an aggregate value. For example, in our first modeling approach, when we used total fantasy points, some of the top players received extremely low predictions for the following season. One example was Saquon Barkley, who is a top running back in the league. One year, he only played in 2 games due to a season ending injury. However, in those two games, he averaged ~15 points per game. In this regard, although he recorded one of the lowest total points for that year, he was one of the best players.\n\n\nAdvanced Data\nWe also pulled advanced player data from the same website, which brings in some more advanced calculations into our dataset. While many of these metrics are important, they are often skipped by the mainstream media due to their complicated nature or low appeal for their audience. Because the two datasets came from the same website, we could use a similar approach for our web-scraping, and the merge was made easier due to matching names. One area which required a little massaging was ensuring we did not have duplicate variables. As you will see in our basic data, there are multiple Td, Yds, Att columns. This represents passing vs rushing statistics. As each position had slightly different data, it became important to us to invest time in cleaning / un-duplicating these features. Additionally, many columns were repeated in the merging process with the advanced dataset. To clean this data in an efficient and organized way, we wrote a bunch of functions in our main class file to help us tackle the problem.\nWe wound up training our model on every year except for the most recent. This allowed us to test our results against the most recent years worth of data. We evaluated our models based on MSE. If a model provided a better MSE than the model we had previously saved as the best, we would update and now return the new type of model. Our biggest hurdle was aquiring enough data to run an effective model as there are only 32 teams and some positions only have 1 that gets points. We had to take several years worth to help us overcome this challenge.\n\n\n\nOur approach\n\nData collection\nA big problem we faced was a lack of data. More specifically, we initially started with just one season of data to make our predictions. This quickly caused problems, as in some positional groups we were left with only ~30 players as observations after cleaning and preparing our data. Therefore, we switched our data source and layered ~10 seasons worth of data onto each positional group. We ended up removing player names as a feature, as this could have ended up being a feature due to repeated values over different years. This left us with hundreds of observations to work with.\n\n\nPreprocessing\nBefore we employed our models, we performed feature selection and normalization techniques. First, because of our merged dataset, we had a copious amount of features to choose from. We relied on sklearn’s SelectKBest algorithm for most of the heavy lifting. Before this process, however, we made sure to standardize our data to ensure the feature selection algorithm did not favor features with naturally larger values. Here is our feature selection function:\n\nfrom sklearn.feature_selection import SelectKBest, f_regression\ndef getBestFeatures(X, y, numFeatures = 5):\n        \n        # Get best features\n        selector = SelectKBest(score_func=f_regression, k=numFeatures)\n        selector.fit(X, y)\n\n        selected_features = X.columns[selector.get_support()]\n\n        X_selected = X[selected_features]\n\n        return X_selected, selected_features\n\nFor each positional group, the 5 selected features were different and unique to that position. For example, 20+ yard receptions are much more important in predicting wide receiver performance than they are for quarterbacks, who pass the ball.\n\n\nModeling\nNext, we performed our modeling. For each position, we tested 8 models on each positions training data and used the one that performed the best. These models included a Linear Regression model, a SGDRegressor model, a Ridge model, a Lasso model, a Decision Tree model, a Random Forest model, a SVR model, and a Multi-Layered Perceptron model. After training, this model was then returned to evaluate the validation data for each position. Once tested, our models were then used to predict fantasy scores for our testing data of the year 2021-2022.\n\n\nPerformance evaluation\nIn our model selection we used MSE as our metric to pick the best model for each position. The MSE for each position varied due to each position having a different average for score. For example, QB’s score the most amount of points in fantasy football whereas TE’s score the least amount of points (besides kickers and defense). In this way, the MSE for QB’s was naturally higher than that of TE’s and this trend was prominent for all positions."
  },
  {
    "objectID": "posts/final-project/index.html#results",
    "href": "posts/final-project/index.html#results",
    "title": "Final Project Blog",
    "section": "Results",
    "text": "Results\nWe were able to complete our goal of making projections for all fantasy players this upcoming season. Sadly, ESPN and other reputable fantasy football sites do not have their performance projections for previous years avaiable to use. Significantly, we wanted to compare our model’s performance against highly used fantasy football projections. Because we were not able to compare our model to other models, it is hard to see whether our model truly performed well. However, we visualized our final testing data for the 2022 year and compared our models projections to the players actual performance. In our pandas tables shown in our source code, it is apparent that our model did a sufficient job at predicting the performance of many players. We were also able to get the average difference between our projection and the players actual performance in 2022. This was a clear sign to us that the model was performing well because for our rookie WR’s, our projections were on average less than 2 points off, QB’s were on average about 6 points off, TE’s were on average less than 1.5 points off, and RB’s were on average about 3 points off. In the context of fantasy football, for players that have no previous data in the league, our model is able to give us reasonably accurate projections that many of these big sites often get wrong."
  },
  {
    "objectID": "posts/final-project/index.html#concluding-discussion",
    "href": "posts/final-project/index.html#concluding-discussion",
    "title": "Final Project Blog",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nYour conclusion is the right time to assess:\nIn what ways did our project work?\nDid we meet the goals that we set at the beginning of the project?\nHow do our results compare to the results of others who have also studied similar problems?\nIf we had more time, data, or computational resources, what might we do differently in order to improve further?\nWe were very happy with the results of our project. We set out to create a model that would allow us to predict fantasy football performance in the future and we were able to accomplish just that. Because of the way we set up our code and the way we scrape data and train our model, it will be very easy to alter our code in future years and allows us to predict future results. We unfortunately have not had time to compare our model with those of major fantasy football platforms, but we are happy enough with our results that we are all comfortable taking our work and applying it to our own fantasy football leagues. If we had time, we would try to add in even more features to train on and we would also add in more ways to test the effectiveness of our model. Overall, we are really happy with what we were able to put out and look forward to continuing work in the future."
  },
  {
    "objectID": "posts/final-project/index.html#group-contributions-statement",
    "href": "posts/final-project/index.html#group-contributions-statement",
    "title": "Final Project Blog",
    "section": "Group Contributions Statement",
    "text": "Group Contributions Statement\nIn your group contributions statement, please include a short paragraph for each group member describing how they contributed to the project:\nWho worked on which parts of the source code?\nWho performed or visualized which experiments?\nWho led the writing of which parts of the blog post?\nEtc.\nEthan Coomber:\nI spent a lot of time working on cleaning data and developing the model. We had to make sure we had sufficient data and I tried to ensure we had clean, usable data. Once I was able to ensure that, I spent my time working on developing a way to choose the best model. This took time as we had to research various models and determine what kind of model would be most effective in helping us predict performance. We then implemented the models we thought had potential and had to have a way to select the best one.\nJohnny Kantaros:\nI spent time initially working on data collection (including the web-scraping), and then spent a lot of time on data cleaning and preprocessing tactics. A large portion of this project was data collection, manipulation, and wrangling, and I definitely learned a lot about the various functionalities of Pandas and other frameworks. Finally, I helped Ethan with adding some models to our modeling function. Our team did a great job working collaboratively so everyone achieved learning in all parts of the pipeline. In terms of this blog post, I wrote the introduction, values statements, and part of the materials + methods sections.\nDean Smith:\nI spent most of my time focusing on the rookie data. A big part of this project was how we would predict scores for rookies who have had no prior data in the league. We concluded that using draft data and team data from the year prior would be the best way to estimate how a rookie would be utilized by their team. Once I gathered and cleaned the data for rookies, my time was spent developing the funciton for feature selection along with integrating the Multi-Layered Perceptron into the model selection. For the blog post, I took charge in writing the Modeling, Performance Evaluation, and Results sections of the blog post."
  },
  {
    "objectID": "posts/final-project/index.html#personal-reflection",
    "href": "posts/final-project/index.html#personal-reflection",
    "title": "Final Project Blog",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nFrom this process, I learned a lot about working within a team. There were many moving parts to this project and I feel that my team did a great job of handling them. Specifically, I felt that our Github workflow went smoothly. In addition, I also learned about some of the challenges that come with making a machine learning model for a real world problem. One of the challenges that is present in real world problems is finding and cleaning the data. I learned that the foundation of machine learniing projects is not necessarily the construction of the model but more importantly the quality and quantity of the data at hand.\nI feel that my group and I achieved our goal comfortably. We ran into issues and had to rebuild our dataset but we managed to build out a sufficient model that we are proud of and hope to use next fantasy football season. As for me personally, I feel that I definetly met my goal of making a big impact with my final project and putting in my best work.\nIn my professional life I will definitely take the teamworking aspects I learned with me. I feel that these skills are universal and can be applied to almost any software related profession. In general, I will take many of the technical skills I learned thorughout this project such as data collection, data cleaning, modeling, and visualizing findings."
  },
  {
    "objectID": "posts/unsupervised-learning-project/index.html",
    "href": "posts/unsupervised-learning-project/index.html",
    "title": "Unsupervised Learning with Linear Algebra Blog",
    "section": "",
    "text": "In this blog, I will first perform image reconstruction on an rbg image of a dog from the internet using linear algebra. Next I will use linear algebra to try and predict an imported karate club social network. You can find my source code here: https://github.com/dmsmith25/dmsmith25.github.io/blob/main/posts/unsupervised-learning-project/unsupervisedLearning.py\nFirst, I will import my source code along with other packages.\n\nimport numpy as np\nimport PIL\nimport urllib\nfrom unsupervisedLearning import UnsupervisedFunctions\nfrom matplotlib import pyplot as plt\n\nfunctions = UnsupervisedFunctions()\n\nNext, I will read in my image from a url.\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\nurl = \"https://canine.org/wp-content/uploads/2023/05/23Graduation_promo-1-copy.jpg\"\n\nimg = read_image(url)\n\nNow I will make an instance of the image in black and white to reduce the size of the image. Lets see below both images and the shape of the black and white image.\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\ngrey_img.shape\n\n(1159, 1500)\n\n\n\n\n\nFinally, lets show the progression of reconstructing the image from a value of k = 1 to k = 33.\n\nfor i in range(1, 40, 10):\n    functions.svd_reconstruct(grey_img, i)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, lets import and see the karate club social network.\n\nimport networkx as nx\nG = nx.karate_club_graph()\nlayout = nx.layout.fruchterman_reingold_layout(G)\nnx.draw(G, layout, with_labels=True, node_color = \"steelblue\")\n\n\n\n\nLets now use the laplacian matrix, eigenvalues, and some linear algebra to predict what the divide in the karate club will look like.\n\nL = nx.normalized_laplacian_matrix(G)\nnormalized_laplacian = L.toarray()\n\nvalues, vectors = np.linalg.eig(normalized_laplacian)\n\nz_ = vectors[:, 1]\n\nz = z_ > 0\n\nnode_colors = np.where(z == True, \"steelblue\", \"orange\")\n\nnx.draw(G, layout, with_labels=True, node_color = node_colors)\n\n\n\n\nTo conclude, through this blog I have presented two processes of unsupervised learning using linear algebra. Image reconstruction is incredibly useful when it comes to managing and storing large datasets of images. Spectral Community Detection is useful when it comes to finding trends amongst networks which has many applications."
  },
  {
    "objectID": "posts/gradient-descent-project/index.html",
    "href": "posts/gradient-descent-project/index.html",
    "title": "Gradient Descent Blog",
    "section": "",
    "text": "In this blog, I will demonstrate how to implement gradient descent along with stochastic gradient descent. Gradient descent is the process in which we adjust the weights of our perceptron model thorugh the equation w = w - alpha * gradient. I will represent my findings using graphs along with explaining my processes throughout the blog.\nHere, I retrieve my data set be importing it through one of sklearn’s datasets. The data set chosen has two features (feature 1 and feature 2) which are displayed in the graph shown after running the code cell below. Additionally, I have also imported my logisticRegression source code which can be found using the following link: https://github.com/dmsmith25/dmsmith25.github.io/blob/7eeef82cbf16398e0d0e9900e31df7dd50b99e12/posts/gradient-descent-project/logisticRegression.py\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom logisticRegression import LogisticRegression\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn the code cell below, I initialize my logisticRegression instance LR using my imported source code. Then, I fit my logisticRegression model to my dataset. In my source code, the fit function uses gradient descent to adjust the weight of the logisticRegression model to accurately classify points from the dataset.\n\nLR = LogisticRegression()\n\nLR.fit(X, y, alpha=0.1, max_epochs=1000)\n\n# inspect the fitted value of w for LR\nLR.w \n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nTo see the performance of the logisticRegression model, the code cell below shows the line formed from the weights of the now fitted model and the emperical risk with respect to iterations in the fitting phase.\n\nloss = LR.prev_loss\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (-f1*LR.w[0])/LR.w[1], color = \"black\")\n\nnum_steps = len(LR.loss_history)\naxarr[1].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n#axarr[1].plot(LRS.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\nFinally, I introduce a modified form of gradient descent called stochastic gradient descent. In stochastic gradient descent, my model does not calculate the full gradient and instead calculates the gradient of batches of my dataset. Visually, the chart below shows the loss with respect to iterations between gradient descent and stochastic gradient descent.\n\nLRS = LogisticRegression()\nLRS.fit_stochastic(X, y, \n                  m_epochs = 1000,  \n                  batch_size = 10, \n                  alpha = 0.1)\n\nnum_steps = len(LRS.loss_history)\nplt.plot(np.arange(num_steps) + 1, LRS.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, \n                  max_epochs = 1000,\n                  alpha = 1.0)\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Iterations\")\nylab = plt.ylabel(\"Loss\")\n\nlegend = plt.legend() \n\n\n\n\nTo conclude, we found that through gradient descent we were able to minimize our emperical risk with our convex loss function sigmoid. We implemented gradient descent on some sample data and represented our findings in graphs. Lastly, we implemented stochastic gradient descent which calculates the partial gradient lightening the computation load on our program."
  },
  {
    "objectID": "posts/perceptron-project/index.html",
    "href": "posts/perceptron-project/index.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Runtime Analysis\nFor each update of the perceptron, it will take O(p) runtime because the number of datapoints is irrelevant when talking about just a singular update. In each update, the runtime is dependant on the number of features or columns of the matricies being multiplied.\nIn my findings, I was able to implement the perceptron algorithm and test it on various types of data. When data is linearly seperable, the perceptron will be able to classify data with 100% accuracy. If data is not linearly seperable then it will find the “line of best fit” for that particular dataset. The perceptron can be used on datasets with many features and the implementation will still stay the same but visualizing results will be increasingly difficult."
  },
  {
    "objectID": "posts/allocative-bias-project/newIndex.html",
    "href": "posts/allocative-bias-project/newIndex.html",
    "title": "Dean Smith's CSCI 0451 Blog",
    "section": "",
    "text": "---\ntitle: Allocative Bias Blog\nauthor: Dean Smith\ndate: '2023-5-9'\nimage: \"penguins.png\"\ndescription: \"In this blog post, I use New Jersey employment data to build a machine learning model and examine the model for potential bias. \"\nformat: html\n---\n# Auditing Allocative Bias\nFirst, I import my dataset and packages I need for my research.\nfrom folktables import ACSDataSource, ACSIncome, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"NJ\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\nThere is a lot of data here. I only want to use relevent features here so I will go ahead and clean my data.\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\nI don't want to include Race or Employment status in my features so I will leave those out and get my features, labels, and group (Race).\nfeatures_to_use = [f for f in possible_features if f not in ['RAC1P', \"ESR\"]]\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\nLets see the shape of our features and the size of our dataset.\nfor obj in [features, label, group]:\n  print(obj.shape)\nNow lets do a test/train split.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\nHere, we put back our group (race) to groupby and our labels (employment status).\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\nlen(df)\nFrom the output of the code cell below, we can see that about 48% of individuals in our training data are employed.\ndf[\"label\"].mean()\nNow, lets see how many people are in each racial group in our training data. Race group 1 is white, race group 2 is black, and the rest are other self-identified racial groups.\ndf.groupby(\"group\").size()\nLikewise, lets see what the employment rate for each racial group is.\ndf.groupby(\"group\")[\"label\"].mean()\nNow lets visualize this data in a bar graph where we also show the different sex groups within the racial groups. Sex group 1 is male and sex group 2 is female.\nimport seaborn as sns\ncounts = df.groupby([\"group\", \"SEX\"])[\"label\"].mean().reset_index(name = \"mean\")\nsns.barplot(data = counts, x = \"group\", y = \"mean\", hue = \"SEX\")\nIn the code cell below, we train our Logistic Regression model and see which number of polynomial features works best.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import PolynomialFeatures\nimport warnings\n\nbest_model = []\nbest_score = -1\nbest_dgegree = 0\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    for deg in range(0, 4):\n\n        polynomial_logistic = Pipeline([\n            ('poly', PolynomialFeatures(degree=deg)),\n            ('logistic', LogisticRegression())\n        ])\n\n        iter_model = polynomial_logistic.fit(X_train, y_train)\n\n        acc = iter_model.score(X_train, y_train)\n\n        print(\"Degree of \" + str(deg) + \" -> Training Accuracy = \" + str(acc))\n\n        if acc > best_score:\n            best_score = acc\n            best_degree = deg\n            best_model.append(iter_model)\n\nprint(\"Best Degree for Model: \" + str(best_degree))\n\nmodel = best_model[-1]\n\nNow lets get our model predictions for our testing data\ny_hat = model.predict(X_test)\nAnd now we see our model had a total accuracy of about 82%!\n(y_hat == y_test).mean()\nBelow, we utilize a confusion matrix to get our model's PPV, FNR, and FPR.\ntn, fp, fn, tp = confusion_matrix(y_test, y_hat, normalize='true').ravel()\nppv = tp / (tp + fp)\nfnr = fn / (fn + tp)\nfpr = fp / (fp + tn)\n\nprint(\"PPV: \" + str(ppv))\nprint(\"FNR: \" + str(fnr))\nprint(\"FPR: \" + str(fpr))\nNow lets see what our model's accuracy, PPV, FNR, and FPR are for both our white racial group and our black racial group.\nfor group in range(1, 10):\n    try:\n        overall_acc = (y_hat == y_test)[group_test == group].mean()\n        test = y_test[group_test == group]\n        pred = y_hat[group_test == group]\n        tn, fp, fn, tp = confusion_matrix(test, pred, normalize='true').ravel()\n        ppv = tp / (tp + fp)\n        fnr = fn / (fn + tp)\n        fpr = fp / (fp + tn)\n\n        print(\"Group \" + str(group) + \" overall accuracy: \" + str(overall_acc)+ \" | PPV: \" + str(ppv) + \" | FNR: \" + str(fnr) + \" | FPR: \" + str(fpr) + \"\\n\")\n    except:\n        print(\"Group \" + str(group) + \" does not have sufficient enough data.\\n\")\n\n\n\nAs we can see, the model performs worse when dealing with black individuals as compared to white individuals. Additionally, the model performs better on white individuals than the total model average and worse on black individuals compared to the models average. However, the model had a higher PPV than the models average when predicting white individuals and a lower PPV when predicting black individuals. This means that when the model guesses that a white person is employed, the model correctly guesses it more than the average, but when the model guesses that a black person is employed, the model predicts worse than the average. In contrast, for black individuals, the model has a slightly higher FNR than the average meaning that when the model predicts a black person is unemployed then the model predicts wrong more than the average while the model's FNR for white indiviuals is about the average. Lastly,  \nHere, we put some data into our test dataframe for graphing later.\nimport pandas as pd\ndf_test = pd.DataFrame(X_test, columns = features_to_use)\ndf_test[\"group\"] = group_test\ndf_test[\"label\"] = y_test\ndf_test[\"prediction\"] = y_hat\n\nlen(df)\nNow lets see if our model is calibrated.\ndf_test = df_test[df_test[\"group\"] < 3]\n\nmeans = df_test.groupby([\"group\", \"prediction\"])[\"label\"].mean().reset_index(name = \"mean\")\nsns.lineplot(data = means, x = \"prediction\", y = \"mean\", hue = \"group\")\ndf_test[\"employed\"] = df_test[\"prediction\"] == 1\n\nmeans = df_test.groupby([\"group\", \"employed\"])[\"label\"].mean().reset_index(name = \"mean\")\n\np = sns.barplot(data = means, x = \"employed\", y = \"mean\", hue = \"group\")"
  },
  {
    "objectID": "posts/allocative-bias-project/index.html",
    "href": "posts/allocative-bias-project/index.html",
    "title": "Allocative Bias Blog",
    "section": "",
    "text": "Concluding Discussion\nWhat groups of people could stand to benefit from a system that is able to predict the label you predicted, such as income or employment status? For example, what kinds of companies might want to buy your model for commercial use?\n\nInsurance companies or banks are the first ones that come to mind that could benefit from a model like this. Any company who benefits from knowing how much if any income is a person making would benefit from a model such as this one.\n\nBased on your bias audit, what could be the impact of deploying your model for large-scale prediction in commercial or governmental settings?\n\nAccording to the data, black individuals are less likely to be employed than white individuals. The model has picked up on this trend however I believe that incorporating this trend into a model used in a large-scale prediction would be detrimental to black individuals as a whole. The model would perpetuate negative biases about black peoples employment which would be a large issue.\n\nBased on your bias audit, do you feel that your model displays problematic bias? What kind (calibration, error rate, etc)?\n\nI believe my model’s error rate reflects the population in the testing data. However I feel that in this data there are biases that the model has picked up on. I also feel that there can be a calibration issue with my model.\n\nBeyond bias, are there other potential problems associated with deploying your model that make you uncomfortable? How would you propose addressing some of these problems?\n\nI feel uncomfortable with the fact that I have only done a limited number of tests for potential biases in my model. There could definitely be more and I would not feel comfortable releasing this model to the world with further testing for biases."
  },
  {
    "objectID": "posts/penguins-project/index.html",
    "href": "posts/penguins-project/index.html",
    "title": "Penguins Blog",
    "section": "",
    "text": "In this blog, I will work with a dataset to classify penguins into 3 species. My dataset contains 17 potential features but in this blog my challenge is to use only 3 of these features to classify penguins at 100% accuracy. The purpose of this blog is to show how one might eliminate irrelevant features to save computation time and still give effective results.\nIn the code cell below, I import my pandas dependency and import my training dataset from a remote github repo.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nTo see the first 5 rows of my training dataset run the code cell below\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\nHere, I import my sklearn.preprocessing pacakge and prepare my data. This process includes hot encoding and dropping unnessecary features such as comments and individual ID.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n\n\n\nIn the code cell below, I import a combinations package to help me find all combinations of features where one is qualitative data and the other two are quantitative data. The purpose of this is to later test which combination of features yields the best performance in the classification model.\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncol_combos = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    col_combos.append(cols)\n  \n\nHere, I determine the best features and the training score of these features.\n\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\n\n\nwarnings.filterwarnings(\"ignore\") # I was getting a lot of warnings\n\nbest_score = 0.0\nbest_cols = [\"N/A\"]\n\nfor col in col_combos:\n    LR = LogisticRegression()\n    LR.fit(X_train[col], y_train)\n    score = LR.score(X_train[col], y_train)\n\n    if score > best_score:\n        best_score = score\n        best_cols = col\n\nprint(best_score)\nprint(best_cols)\n\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nNow, I get my model with the best features and train it for the testing phase.\n\nbest_cols = best_cols[(len(best_cols) - 2):] + best_cols[:(len(best_cols) - 2)]\n\nLR = LogisticRegression()\nLR.fit(X_train[best_cols], y_train)\nscore = LR.score(X_train[best_cols], y_train)\n\nprint(score)\n\n1.0\n\n\nIn the code cell below, I test my model against an imported testing dataset and print the testing score.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nIn the next two code cells, I prep the training data to only include the best features in order to show them visually.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nX_train[best_cols]\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      1\n      0\n      0\n    \n    \n      2\n      41.4\n      18.5\n      0\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      0\n      1\n      0\n    \n    \n      4\n      50.6\n      19.4\n      0\n      1\n      0\n    \n    \n      5\n      33.1\n      16.1\n      0\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      0\n      1\n      0\n    \n    \n      270\n      45.4\n      14.6\n      1\n      0\n      0\n    \n    \n      271\n      36.2\n      17.2\n      0\n      0\n      1\n    \n    \n      272\n      50.0\n      15.9\n      1\n      0\n      0\n    \n    \n      273\n      48.2\n      14.3\n      1\n      0\n      0\n    \n  \n\n256 rows × 5 columns\n\n\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFinally, we are able to visualize the performance of this model.\n\nplot_regions(LR, X_train[best_cols], y_train)\n\n\n\n\nIn our findings, we determined that Island, Culmen Length, and Culmen Depth were the best combination of features to classify penguins into the Adelie, Chinstrap, and Gentoo species. We were able to obtain a 100% training accuracy along with a 100% testing accuracy. Lastly, we represented the data visually and can see that our model effectively classifies penguins into these species."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dean Smith’s CSCI 0451 Blog",
    "section": "",
    "text": "In this blog I present my group’s final project where we build a machine learning model to predict the performence of fantasy football players.\n\n\n\n\n\n\nMay 20, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog I will use linear algebra to perform unsupervised learning processes.\n\n\n\n\n\n\nMay 9, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog post, I use New Jersey employment data to build a machine learning model and examine the model for potential bias.\n\n\n\n\n\n\nMay 9, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog I show the effectiveness of least-squares linear regression and LASSO regularization\n\n\n\n\n\n\nMay 2, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA reflection of what I have learned from Timnit Gebru’s experience and talk with our class.\n\n\n\n\n\n\nApr 19, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog I build a model to classify different penguins\n\n\n\n\n\n\nApr 18, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog I build and implement a classification model using gradient descent\n\n\n\n\n\n\nApr 7, 2023\n\n\nDean Smith\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog I build and implement a simple perceptron model\n\n\n\n\n\n\nFeb 22, 2023\n\n\nDean Smith\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I made this blog in my CSCI 451 class to share my projects on Machine Learning topics."
  }
]